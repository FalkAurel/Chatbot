services:
  frontend:
    build: "./frontend"
    image: frontend:latest
    ports:
      - "8501:8501"
    networks:
      - private_network
    depends_on:
      - backend
      - ml_pipeline
    restart: unless-stopped
  
  backend:
    build: "./backend"
    image: backend:latest
    networks:
      - private_network
    depends_on:
      - ml_pipeline
    ports:
      - "8080:8080"
    volumes:
      - backend_data:/app/data
    restart: unless-stopped
    
  ml_pipeline:
    build: "./ml_pipeline"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      - RUST_BACKTRACE=full
      - SURREAL_URL=surrealdb:8000
      - OLLAMA_MODELL=gemma3:12b
    image: ml_pipeline:latest
    networks:
      - private_network
    ports:
      - "3030:3030"
    depends_on:
      surrealdb:
        condition: service_started
    volumes:
      - ml_pipeline_data:/app/data
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  ollama:
    image: docker.1ms.run/ollama/ollama:latest
    command: serve
    environment:
      - OLLAMA_KV_CACHE_TYPE=q8_0
      - OLLAMA_KEEP_ALIVE=120m
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - private_network
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  
  surrealdb:
    image: docker.1ms.run/surrealdb/surrealdb:latest
    command: start --user root --pass root --log info --bind 0.0.0.0:8000 file:/data/database.db # Give it sudo rights in the fucking container
    networks:
      - private_network
    ports:
      - "8000:8000"
    volumes:
      - surrealdb_data:/data
    restart: unless-stopped
    user: "root"

networks:
  private_network:
    driver: bridge

volumes:
  backend_data: {}
  ml_pipeline_data: {}
  ollama_data: {}
  surrealdb_data: {}

